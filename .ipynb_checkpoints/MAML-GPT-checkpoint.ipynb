{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b909e49-7406-4e1b-b87e-8f7852b549db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\Grid\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import transformers\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ef63d6c-62fc-4a39-a124-b0f236297534",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Credits\n",
    "## This cell includes code from StylePTB (https://github.com/lvyiwei1/StylePTB/tree/master) by Yiwei Lyu et al., \n",
    "## available under the Creative Commons Attribution 4.0 International License ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)).\n",
    "\n",
    "def lowering(pairs):\n",
    "    for pair in pairs:\n",
    "        for i in range(0, 2):\n",
    "            pair[i] = pair[i].lower()\n",
    "\n",
    "def numpreprocess(pairs):\n",
    "    for pair in pairs:\n",
    "        for i in range(0, 2):\n",
    "            rep = []\n",
    "            for word in pair[i].split(' '):\n",
    "                if len(word) > 0 and word[0] in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n",
    "                    rep.append(\"NUM\")\n",
    "                else:\n",
    "                    rep.append(word)\n",
    "            pair[i] = ' '.join(rep)\n",
    "\n",
    "def padinput(inputlist, totalpad=80):\n",
    "    pads = [0] * (totalpad - len(inputlist))\n",
    "    input = inputlist + pads\n",
    "    mask = [1] * len(inputlist) + pads\n",
    "    return input, mask\n",
    "\n",
    "# create label for training\n",
    "def labels(inlen, outputlist, totalpad=80):\n",
    "    pads1 = [-100] * inlen\n",
    "    pads2 = [-100] * (totalpad - inlen - len(outputlist))\n",
    "    # print(outputlist)\n",
    "    return pads1 + outputlist + pads2\n",
    "\n",
    "def batchvalid(src, trg, batchsize):\n",
    "    validloss = 0.0\n",
    "    for i in range(0, len(src) // batchsize):\n",
    "        asrc = []\n",
    "        atrg = []\n",
    "        for pair in src[i * batchsize:(i + 1) * batchsize]:\n",
    "            asrc.append(pair)\n",
    "        for pair in trg[i * batchsize:(i + 1) * batchsize]:\n",
    "            atrg.append(pair)\n",
    "        validloss += valid(asrc, atrg)\n",
    "    return validloss / (len(src) // batchsize)\n",
    "\n",
    "def valid(src, trg):\n",
    "    padin = [padinput(l) for l in src]\n",
    "    padedin = torch.LongTensor([padin[i][0] for i in range(0, len(trg))]).to(device)\n",
    "    masks = torch.LongTensor([padin[i][1] for i in range(0, len(trg))]).to(device)\n",
    "    label = torch.LongTensor([labels(len(src[i]), trg[i]) for i in range(0, len(trg))]).to(device)\n",
    "    with torch.no_grad():\n",
    "        ret = gpt_model.forward(padedin, attention_mask=masks, labels=label)\n",
    "        loss = ret[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3caf2c59-257f-45ef-ba64-bca2d8318060",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAML_GPT():\n",
    "    def __init__(self, gpt_model, tasks, gpt_tokenizer=None, inner_lr=2e-4, meta_lr=2e-5, K=10, \n",
    "                 multi_batch_iter=1, inner_steps=1, epochs=10000, early_stop=50, model_save_name=\"maml_gpt\"):\n",
    "        self.tasks = tasks\n",
    "        self.model = gpt_model\n",
    "        self.gpt_tokenizer = gpt_tokenizer\n",
    "        #self.criterion = nn.MSELoss()\n",
    "        self.meta_optimiser = optim.Adam(model.parameters(), meta_lr)\n",
    "        self.inner_lr = inner_lr\n",
    "        self.meta_lr = meta_lr\n",
    "        self.K = K\n",
    "        self.inner_steps = inner_steps\n",
    "        self.plot_every = 1\n",
    "        self.print_every = 10\n",
    "        self.meta_losses = []\n",
    "        self.epochs = epochs\n",
    "        self.early_stop = early_stop\n",
    "        self.meta_batch_size = len(tasks)\n",
    "        self.model_save_name = model_save_name\n",
    "        self.multi_batch_iter = multi_batch_iter\n",
    "        \n",
    "    def inner_loop(self, task):\n",
    "        with higher.innerloop_ctx(self.model, self.meta_optimiser, copy_initial_weights=True) as (fmodel, diffopt):\n",
    "            #X, y = task.sample_data(self.K) #TODO\n",
    "            random_selected_samples = random.sample(task, 2*self.K)\n",
    "            for step in range(self.inner_steps):\n",
    "                pred = fmodel(X)\n",
    "                loss = pred[0]\n",
    "                diffopt.step(loss)\n",
    "            \n",
    "            X, y = task.sample_data(self.K) #TODO\n",
    "            pred = fmodel(X)\n",
    "            loss = pred[0]\n",
    "            return loss\n",
    "    \n",
    "    def main_loop(self, num_iterations):\n",
    "        min_loss = 999\n",
    "        early_stop_count = 0\n",
    "        print_loss = 0\n",
    "        for iteration in range(1, num_iterations + 1):\n",
    "            meta_loss = 0\n",
    "            for _ in range(multi_batch_iter):\n",
    "                for task in tasks:\n",
    "                    meta_loss += self.inner_loop(task)\n",
    "            if meta_loss < min_loss:\n",
    "                min_loss = meta_loss\n",
    "                early_stop_count = 0\n",
    "                print(f\"New lowest loss found!\")\n",
    "                torch.save(self.model, f\".\\\\MAML_GPT_models\\\\{self.model_save_name}_epoch{iteration}.pt\")\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "                if early_stop_count > self.early_stop:\n",
    "                    print(f\"Early stop at epoch {iteration} because no lower loss is found in {early_stop} epochs\"}\n",
    "                    return\n",
    "\n",
    "            self.meta_optimiser.zero_grad()\n",
    "            meta_loss.backward()\n",
    "            self.meta_optimiser.step()\n",
    "            print_loss += meta_loss.item() / self.meta_batch_size\n",
    "            if iteration % self.print_every == 0:\n",
    "                print(f\"Epoch {iteration}/{num_iterations}. loss: {print_loss / print_every}\")\n",
    "                print_loss = 0\n",
    "            if iteration % self.plot_every == 0:\n",
    "                self.meta_losses.append(meta_loss.item() / self.meta_batch_size)\n",
    "\n",
    "    def train(self):\n",
    "        self.main_loop(self.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e9f4364-1c2c-44cf-9515-10c5a39252f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "# Pre-process the data\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Device: {device}\")\n",
    "gpt_tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "meta_train_task_name_list = [\"ARR\", \"TFU\", \"ATP\", \"PPR\", \"TPA\"]\n",
    "meta_test_task_name_list = [\"PTA\", \"SBR\", \"TPR\"]\n",
    "\n",
    "train_task_pair_list = [] # Nested list with pairs from different tasks\n",
    "for meta_train_task_name in meta_train_task_name_list:\n",
    "    f = open(f'.\\\\Data\\\\Meta_training\\\\{meta_train_task_name}\\\\train.tsv', 'r')\n",
    "    ff = csv.reader(f, delimiter='\\t')\n",
    "    pairs = []\n",
    "    for row in ff:\n",
    "        pairs.append(row)\n",
    "    lowering(pairs)\n",
    "    numpreprocess(pairs)\n",
    "    pairsEncode = []\n",
    "    for i in pairs:\n",
    "        pairsEncode.append((gpt_tokenizer.encode(i[0] + \" <|endoftext|>\"), gpt_tokenizer.encode(i[1] + \" <|endoftext|>\")))\n",
    "    train_task_pair_list.append(pairsEncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d6ae7e-b65e-4ba6-b546-4447a3bdac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "maml_gpt = MAML_GPT(gpt_model=gpt_model, tasks=train_task_pair_list, gpt_tokenizer=gpt_tokenizer, \n",
    "                    multi_batch_iter=1, early_stop=50, model_save_name=\"maml_gpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
