{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b909e49-7406-4e1b-b87e-8f7852b549db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:52.151835Z",
     "start_time": "2024-03-26T02:14:51.065430Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\Grid\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import transformers\n",
    "import torch\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import higher\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef63d6c-62fc-4a39-a124-b0f236297534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:53.197113Z",
     "start_time": "2024-03-26T02:14:53.185114Z"
    }
   },
   "outputs": [],
   "source": [
    "## Credits\n",
    "## This cell includes code from StylePTB (https://github.com/lvyiwei1/StylePTB/tree/master) by Yiwei Lyu et al., \n",
    "## available under the Creative Commons Attribution 4.0 International License ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)).\n",
    "\n",
    "def lowering(pairs):\n",
    "    for pair in pairs:\n",
    "        for i in range(0, 2):\n",
    "            pair[i] = pair[i].lower()\n",
    "\n",
    "def numpreprocess(pairs):\n",
    "    for pair in pairs:\n",
    "        for i in range(0, 2):\n",
    "            rep = []\n",
    "            for word in pair[i].split(' '):\n",
    "                if len(word) > 0 and word[0] in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n",
    "                    rep.append(\"NUM\")\n",
    "                else:\n",
    "                    rep.append(word)\n",
    "            pair[i] = ' '.join(rep)\n",
    "\n",
    "def padinput(inputlist, totalpad=80):\n",
    "    pads = [0] * (totalpad - len(inputlist))\n",
    "    input = inputlist + pads\n",
    "    mask = [1] * len(inputlist) + pads\n",
    "    return input, mask\n",
    "\n",
    "# create label for training\n",
    "def labels(inlen, outputlist, totalpad=80):\n",
    "    pads1 = [-100] * inlen\n",
    "    pads2 = [-100] * (totalpad - inlen - len(outputlist))\n",
    "    # print(outputlist)\n",
    "    return pads1 + outputlist + pads2\n",
    "\n",
    "def batchvalid(src, trg, batchsize):\n",
    "    validloss = 0.0\n",
    "    for i in range(0, len(src) // batchsize):\n",
    "        asrc = []\n",
    "        atrg = []\n",
    "        for pair in src[i * batchsize:(i + 1) * batchsize]:\n",
    "            asrc.append(pair)\n",
    "        for pair in trg[i * batchsize:(i + 1) * batchsize]:\n",
    "            atrg.append(pair)\n",
    "        validloss += valid(asrc, atrg)\n",
    "    return validloss / (len(src) // batchsize)\n",
    "\n",
    "def valid(src, trg):\n",
    "    padin = [padinput(l) for l in src]\n",
    "    padedin = torch.LongTensor([padin[i][0] for i in range(0, len(trg))]).to(device)\n",
    "    masks = torch.LongTensor([padin[i][1] for i in range(0, len(trg))]).to(device)\n",
    "    label = torch.LongTensor([labels(len(src[i]), trg[i]) for i in range(0, len(trg))]).to(device)\n",
    "    with torch.no_grad():\n",
    "        ret = gpt_model.forward(padedin, attention_mask=masks, labels=label)\n",
    "        loss = ret[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3caf2c59-257f-45ef-ba64-bca2d8318060",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:53.766296Z",
     "start_time": "2024-03-26T02:14:53.751233Z"
    }
   },
   "outputs": [],
   "source": [
    "class MAML_GPT():\n",
    "    def __init__(self, gpt_model, tasks, device, gpt_tokenizer=None, inner_lr=2e-4, meta_lr=2e-5, K=10, \n",
    "                 multi_batch_iter=1, inner_steps=1, early_stop=50, model_save_name=\"maml_gpt\"):\n",
    "        self.tasks = tasks\n",
    "        self.model = gpt_model\n",
    "        self.gpt_tokenizer = gpt_tokenizer\n",
    "        #self.criterion = nn.MSELoss()\n",
    "        self.meta_optimiser = optim.Adam(self.model.parameters(), meta_lr)\n",
    "        self.inner_lr = inner_lr\n",
    "        self.meta_lr = meta_lr\n",
    "        self.K = K\n",
    "        self.inner_steps = inner_steps\n",
    "        self.plot_every = 1\n",
    "        self.print_every = 1\n",
    "        self.meta_losses = []\n",
    "        self.early_stop = early_stop\n",
    "        self.meta_batch_size = len(tasks)\n",
    "        self.model_save_name = model_save_name\n",
    "        self.multi_batch_iter = multi_batch_iter\n",
    "        self.device = device\n",
    "        \n",
    "    def inner_loop(self, task):\n",
    "        self.inner_loop_counter += 1\n",
    "        \n",
    "        start_inner_time = time.time()  # 开始计时\n",
    "        if torch.cuda.is_available():\n",
    "            start_inner_mem = torch.cuda.memory_allocated()  # 获取开始时的显存使用情况\n",
    "        print(f\"Start inner loop {self.inner_loop_counter}.\")\n",
    "        print(f\"Current memory used: {(start_inner_mem) / (1024 ** 2)} MB.\")\n",
    "\n",
    "        with higher.innerloop_ctx(self.model, self.inner_opt, copy_initial_weights=False) as (fmodel, diffopt):\n",
    "        \n",
    "            copy_time = time.time()\n",
    "            if torch.cuda.is_available():\n",
    "                copy_mem = torch.cuda.memory_allocated()  # 获取显存使用情况\n",
    "            print(f\"Copy model.\")    \n",
    "            print(f\"Time taken: {copy_time - start_inner_time} seconds; Memory used: {(copy_mem - start_inner_mem) / (1024 ** 2)} MB.\")\n",
    "        \n",
    "            random_selected_samples = random.sample(task, 2*self.K)\n",
    "            inner_train_samples = random_selected_samples[:self.K]\n",
    "            inner_test_samples = random_selected_samples[self.K:]\n",
    "            padedin, masks, labels = self.samples_to_padedin_masks_labels(inner_train_samples)\n",
    "            for step in range(self.inner_steps):\n",
    "                pred = fmodel(padedin, attention_mask=masks, labels=labels)\n",
    "                loss_inner_step = pred[0]\n",
    "                diffopt.step(loss_inner_step)\n",
    "                del loss_inner_step\n",
    "\n",
    "            end_inner_train_time = time.time()\n",
    "            if torch.cuda.is_available():\n",
    "                end_inner_train_mem = torch.cuda.memory_allocated()  # 获取显存使用情况\n",
    "            print(f\"Finish training copied model.\")    \n",
    "            print(f\"Time taken: {end_inner_train_time - copy_time} seconds; Memory used: {(end_inner_train_mem - copy_mem) / (1024 ** 2)} MB.\")\n",
    "        \n",
    "            padedin, masks, labels = self.samples_to_padedin_masks_labels(inner_test_samples)\n",
    "            pred = fmodel(padedin, attention_mask=masks, labels=labels)\n",
    "            loss = pred[0]\n",
    "            loss.backward()\n",
    "            loss_item = loss.detach().item()\n",
    "\n",
    "            end_inner_test_time = time.time()\n",
    "            if torch.cuda.is_available():\n",
    "                end_inner_test_mem = torch.cuda.memory_allocated()  # 获取显存使用情况\n",
    "            print(f\"Finish testing copied model.\")    \n",
    "            print(f\"Time taken: {end_inner_test_time - end_inner_train_time} seconds; Memory used: {(end_inner_test_mem - end_inner_train_mem) / (1024 ** 2)} MB.\")\n",
    "\n",
    "            del loss\n",
    "            del fmodel\n",
    "            del diffopt\n",
    "            \n",
    "            \n",
    "            end_inner_del_time = time.time()\n",
    "            if torch.cuda.is_available():\n",
    "                end_inner_del_mem = torch.cuda.memory_allocated()  # 获取显存使用情况\n",
    "            print(f\"Release Memory.\")    \n",
    "            print(f\"Time taken: {end_inner_del_time - end_inner_test_time} seconds; Memory used: {(end_inner_del_mem - end_inner_test_mem) / (1024 ** 2)} MB.\")\n",
    "\n",
    "            return loss_item\n",
    "    \n",
    "    def main_loop(self, num_iterations):\n",
    "        min_loss = 999\n",
    "        early_stop_count = 0\n",
    "        print_loss = 0\n",
    "        for iteration in range(1, num_iterations + 1):\n",
    "            self.main_loop_counter = iteration\n",
    "\n",
    "            start_main_time = time.time()  # 开始计时\n",
    "            if torch.cuda.is_available():\n",
    "                start_main_mem = torch.cuda.memory_allocated()  # 获取开始时的显存使用情况\n",
    "            print()\n",
    "            print(f\"Start main loop epoch {iteration}.\")\n",
    "            print(f\"Current memory used: {(start_main_mem) / (1024 ** 2)} MB.\")\n",
    "\n",
    "            self.meta_optimiser.zero_grad()\n",
    "            self.inner_opt = torch.optim.SGD(self.model.parameters(), lr=self.inner_lr)\n",
    "            meta_loss = 0\n",
    "            self.inner_loop_counter = 0\n",
    "            for _ in range(self.multi_batch_iter):\n",
    "                for task in self.tasks:\n",
    "                    meta_loss += self.inner_loop(task)\n",
    "            if meta_loss < min_loss:\n",
    "                min_loss = meta_loss\n",
    "                early_stop_count = 0\n",
    "                print(f\"New lowest loss ({meta_loss / (self.meta_batch_size * self.multi_batch_iter)}) found!\")\n",
    "                torch.save(self.model, f\".\\\\MAML_GPT_models\\\\{self.model_save_name}_epoch{iteration}.pt\")\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "                if early_stop_count > self.early_stop:\n",
    "                    print(f\"Early stop at epoch {iteration} because no lower loss is found in {self.early_stop} epochs\")\n",
    "                    return\n",
    "\n",
    "            finish_all_inner_time = time.time()\n",
    "            if torch.cuda.is_available():\n",
    "                finish_all_inner_mem = torch.cuda.memory_allocated()  # 获取显存使用情况\n",
    "            print(f\"Finish all inner loops in this epoch.\")    \n",
    "            print(f\"Time taken: {finish_all_inner_time - start_main_time} seconds; Memory used: {(finish_all_inner_mem - start_main_mem) / (1024 ** 2)} MB.\")\n",
    "            \n",
    "            #self.meta_optimiser.zero_grad()\n",
    "            #meta_loss.backward()\n",
    "            self.meta_optimiser.step()\n",
    "\n",
    "            finish_meta_update_time = time.time()\n",
    "            if torch.cuda.is_available():\n",
    "                finish_meta_update_mem = torch.cuda.memory_allocated()  # 获取显存使用情况\n",
    "            print(f\"Finish meta update in this epoch.\")    \n",
    "            print(f\"Time taken: {finish_meta_update_time - finish_all_inner_time} seconds; Memory used: {(finish_meta_update_mem - finish_all_inner_mem) / (1024 ** 2)} MB.\")\n",
    "            \n",
    "            print_loss += meta_loss / (self.meta_batch_size * self.multi_batch_iter)\n",
    "            if iteration % self.print_every == 0:\n",
    "                print(f\"Epoch {iteration}/{num_iterations}. loss: {print_loss / self.print_every}\")\n",
    "                print_loss = 0\n",
    "            if iteration % self.plot_every == 0:\n",
    "                self.meta_losses.append(meta_loss / (self.meta_batch_size * self.multi_batch_iter))\n",
    "\n",
    "    def start_train(self, epochs=10000):\n",
    "        self.epochs = epochs\n",
    "        self.main_loop(self.epochs)\n",
    "\n",
    "    def samples_to_padedin_masks_labels(self, samples):\n",
    "        src = []\n",
    "        trg = []\n",
    "        for sample in samples:\n",
    "            src.append(sample[0])\n",
    "            trg.append(sample[1])\n",
    "        padin = [self.padinput(l) for l in src]\n",
    "        padedin = torch.LongTensor([padin[i][0] for i in range(0, len(trg))]).to(self.device)\n",
    "        masks = torch.LongTensor([padin[i][1] for i in range(0, len(trg))]).to(self.device)\n",
    "        labels = torch.LongTensor([self.create_labels(len(src[i]), trg[i]) for i in range(0, len(trg))]).to(self.device)\n",
    "        return padedin, masks, labels\n",
    "\n",
    "    def create_labels(self, inlen, outputlist, totalpad=80):\n",
    "        pads1 = [-100] * inlen\n",
    "        pads2 = [-100] * (totalpad - inlen - len(outputlist))\n",
    "        return pads1 + outputlist + pads2\n",
    "    \n",
    "    def padinput(self, inputlist, totalpad=80):\n",
    "        pads = [0] * (totalpad - len(inputlist))\n",
    "        input = inputlist + pads\n",
    "        mask = [1] * len(inputlist) + pads\n",
    "        return input, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e9f4364-1c2c-44cf-9515-10c5a39252f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:57.207614Z",
     "start_time": "2024-03-26T02:14:53.939349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "# Pre-process the data\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Device: {device}\")\n",
    "gpt_tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "meta_train_task_name_list = [\"ARR\", \"TFU\", \"ATP\", \"PPR\", \"TPA\"]\n",
    "meta_test_task_name_list = [\"PTA\", \"SBR\", \"TPR\"]\n",
    "\n",
    "train_task_pair_list = [] # Nested list with pairs from different tasks\n",
    "for meta_train_task_name in meta_train_task_name_list:\n",
    "    f = open(f'.\\\\Data\\\\Meta_training\\\\{meta_train_task_name}\\\\train.tsv', 'r')\n",
    "    ff = csv.reader(f, delimiter='\\t')\n",
    "    pairs = []\n",
    "    for row in ff:\n",
    "        pairs.append(row)\n",
    "    lowering(pairs)\n",
    "    numpreprocess(pairs)\n",
    "    pairsEncode = []\n",
    "    for i in pairs:\n",
    "        pairsEncode.append((gpt_tokenizer.encode(i[0] + \" <|endoftext|>\"), gpt_tokenizer.encode(i[1] + \" <|endoftext|>\")))\n",
    "    train_task_pair_list.append(pairsEncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d6ae7e-b65e-4ba6-b546-4447a3bdac2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:57.238728Z",
     "start_time": "2024-03-26T02:14:57.220617Z"
    }
   },
   "outputs": [],
   "source": [
    "maml_gpt = MAML_GPT(gpt_model=gpt_model, tasks=train_task_pair_list, device = device, gpt_tokenizer=gpt_tokenizer, \n",
    "                    multi_batch_iter=1, early_stop=50, model_save_name=\"maml_gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d543cd5-c31d-4dd3-a94b-863ba853e8f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:30:28.857492Z",
     "start_time": "2024-03-26T02:30:01.736221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start main loop epoch 1.\n",
      "Current memory used: 487.46875 MB.\n",
      "Start inner loop 1.\n",
      "Current memory used: 487.46875 MB.\n",
      "Copy model.\n",
      "Time taken: 0.029909372329711914 seconds; Memory used: 487.46875 MB.\n",
      "Finish training copied model.\n",
      "Time taken: 2.90075421333313 seconds; Memory used: 2887.5654296875 MB.\n",
      "Finish testing copied model.\n",
      "Time taken: 0.4064638614654541 seconds; Memory used: -1675.8125 MB.\n",
      "Release Memory.\n",
      "Time taken: 0.0 seconds; Memory used: 0.0 MB.\n",
      "Start inner loop 2.\n",
      "Current memory used: 983.345703125 MB.\n",
      "Copy model.\n",
      "Time taken: 0.009004831314086914 seconds; Memory used: 495.4296875 MB.\n",
      "Finish training copied model.\n",
      "Time taken: 0.046881675720214844 seconds; Memory used: 2869.0029296875 MB.\n",
      "Finish testing copied model.\n",
      "Time taken: 0.40881800651550293 seconds; Memory used: -2154.6845703125 MB.\n",
      "Release Memory.\n",
      "Time taken: 0.0 seconds; Memory used: 0.0 MB.\n",
      "Start inner loop 3.\n",
      "Current memory used: 983.345703125 MB.\n",
      "Copy model.\n",
      "Time taken: 0.03123760223388672 seconds; Memory used: 495.4296875 MB.\n",
      "Finish training copied model.\n",
      "Time taken: 0.03165578842163086 seconds; Memory used: 2869.0029296875 MB.\n",
      "Finish testing copied model.\n",
      "Time taken: 0.41972923278808594 seconds; Memory used: -2154.6845703125 MB.\n",
      "Release Memory.\n",
      "Time taken: 0.0 seconds; Memory used: 0.0 MB.\n",
      "Start inner loop 4.\n",
      "Current memory used: 983.345703125 MB.\n",
      "Copy model.\n",
      "Time taken: 0.015136241912841797 seconds; Memory used: 495.4296875 MB.\n",
      "Finish training copied model.\n",
      "Time taken: 0.047640323638916016 seconds; Memory used: 2869.0029296875 MB.\n",
      "Finish testing copied model.\n",
      "Time taken: 0.40576601028442383 seconds; Memory used: -2154.6845703125 MB.\n",
      "Release Memory.\n",
      "Time taken: 0.0 seconds; Memory used: 0.0 MB.\n",
      "Start inner loop 5.\n",
      "Current memory used: 983.345703125 MB.\n",
      "Copy model.\n",
      "Time taken: 0.015640974044799805 seconds; Memory used: 495.4296875 MB.\n",
      "Finish training copied model.\n",
      "Time taken: 0.051525115966796875 seconds; Memory used: 2869.0029296875 MB.\n",
      "Finish testing copied model.\n",
      "Time taken: 0.40897703170776367 seconds; Memory used: -2154.6845703125 MB.\n",
      "Release Memory.\n",
      "Time taken: 0.0010089874267578125 seconds; Memory used: 0.0 MB.\n",
      "New lowest loss (9.921358489990235) found!\n",
      "Finish all inner loops in this epoch.\n",
      "Time taken: 5.813799858093262 seconds; Memory used: 495.876953125 MB.\n",
      "Finish meta update in this epoch.\n",
      "Time taken: 0.012230634689331055 seconds; Memory used: 959.66796875 MB.\n",
      "Epoch 1/50. loss: 9.921358489990235\n",
      "\n",
      "Start main loop epoch 2.\n",
      "Current memory used: 1943.013671875 MB.\n",
      "Start inner loop 1.\n",
      "Current memory used: 1463.38671875 MB.\n",
      "Copy model.\n",
      "Time taken: 0.023267745971679688 seconds; Memory used: 490.9375 MB.\n",
      "Finish training copied model.\n",
      "Time taken: 0.04919171333312988 seconds; Memory used: 2870.068359375 MB.\n",
      "Finish testing copied model.\n",
      "Time taken: 0.45386743545532227 seconds; Memory used: -1677.3076171875 MB.\n",
      "Release Memory.\n",
      "Time taken: 0.0 seconds; Memory used: 0.0 MB.\n",
      "Start inner loop 2.\n",
      "Current memory used: 1942.865234375 MB.\n",
      "Copy model.\n",
      "Time taken: 0.01564192771911621 seconds; Memory used: 491.578125 MB.\n",
      "Finish training copied model.\n",
      "Time taken: 0.061121225357055664 seconds; Memory used: 2869.7529296875 MB.\n",
      "Finish testing copied model.\n",
      "Time taken: 0.6988120079040527 seconds; Memory used: -2154.8330078125 MB.\n",
      "Release Memory.\n",
      "Time taken: 0.0 seconds; Memory used: 0.0 MB.\n",
      "Start inner loop 3.\n",
      "Current memory used: 1942.865234375 MB.\n",
      "Copy model.\n",
      "Time taken: 0.018631696701049805 seconds; Memory used: 491.578125 MB.\n",
      "Finish training copied model.\n",
      "Time taken: 0.04426431655883789 seconds; Memory used: 2870.0185546875 MB.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 6.25 GiB is allocated by PyTorch, and 953.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmaml_gpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_train\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 137\u001b[0m, in \u001b[0;36mMAML_GPT.start_train\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m):\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m epochs\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 100\u001b[0m, in \u001b[0;36mMAML_GPT.main_loop\u001b[1;34m(self, num_iterations)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_batch_iter):\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks:\n\u001b[1;32m--> 100\u001b[0m         meta_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta_loss \u001b[38;5;241m<\u001b[39m min_loss:\n\u001b[0;32m    102\u001b[0m     min_loss \u001b[38;5;241m=\u001b[39m meta_loss\n",
      "Cell \u001b[1;32mIn[3], line 58\u001b[0m, in \u001b[0;36mMAML_GPT.inner_loop\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m     56\u001b[0m pred \u001b[38;5;241m=\u001b[39m fmodel(padedin, attention_mask\u001b[38;5;241m=\u001b[39mmasks, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m pred[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 58\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m loss_item \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     61\u001b[0m end_inner_test_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\Grid\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\Grid\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 6.25 GiB is allocated by PyTorch, and 953.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "maml_gpt.start_train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431024c-804e-4902-83dd-079967e05375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
