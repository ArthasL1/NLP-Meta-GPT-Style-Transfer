{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "# import bleu score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "# import transformer model\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_shot_evaluation(model, k_shot, n_samples, optim, num_steps=100):\n",
    "    \"\"\"\n",
    "    Evaluate a model using k-shot learning.\n",
    "    \n",
    "    Args:\n",
    "        model: a model that implements the k_shot_learning method\n",
    "        k_shot:  examples to use for training\n",
    "        n_samples:  examples to use for testing\n",
    "        num_episodes: the number of episodes to run\n",
    "    \"\"\"\n",
    "\n",
    "    gpt_model = torch.load(model)\n",
    "    gpt_tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    '''Load data'''\n",
    "    \n",
    "        # load k-shot data\n",
    "            #k_shot = read_data(k_shot)\n",
    "            #k_input =...\n",
    "            #k_mask =...\n",
    "            #k_label =...\n",
    "            #K = len(k_input)\n",
    "            \n",
    "        # load n-samples data\n",
    "            #n_samples = read_data(n_samples)\n",
    "            #n_input =...\n",
    "            #n_mask =...\n",
    "            #n_label =...\n",
    "            #N = len(n_input)\n",
    "    \n",
    "    \n",
    "    ''' K-shot learning, Train model'''\n",
    "    \n",
    "        # test losses\n",
    "        \n",
    "        # for each step\n",
    "            # test (1st test is the zero-shot learning)\n",
    "            # test bleu score\n",
    "            # test loss\n",
    "            \n",
    "            # train\n",
    "            # train loss\n",
    "            # optimize\n",
    "\n",
    "        # plot losses\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    test_losses = []\n",
    "    bleu_scores = []\n",
    "    for i in range(num_steps):\n",
    "        test_loss = 0\n",
    "        # test, first test is the zero-shot learning\n",
    "        with torch.no_grad():\n",
    "            for N, (n_inputs, n_label, n_masks) in enumerate(n_samples):\n",
    "                ret = model.forward(n_input, attention_mask=n_mask, labels=n_label)\n",
    "                test_loss = ret[0]\n",
    "                test_loss += test_loss.item()\n",
    "                \n",
    "                # get actual text and predicted text\n",
    "                y_text, pred_text = y_pred_text(ret, n_inputs, n_label, gpt_tokenizer)\n",
    "                \n",
    "                # test bleu score\n",
    "                bleu_score = sentence_bleu(y_text, pred_text)\n",
    "                bleu_scores.append(bleu_score)\n",
    "\n",
    "\n",
    "            avg_test_loss = test_loss / len(n_samples)\n",
    "            test_losses.append(avg_test_loss)\n",
    "    \n",
    "    \n",
    "        # train, train K examples\n",
    "        for K, (k_inputs, k_label, k_masks) in enumerate(k_shot):\n",
    "            optimizer.zero_grad()\n",
    "            ret = model.forward(k_input, attention_mask=k_mask, labels=k_label)\n",
    "            train_loss = ret[0]\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    \n",
    "    # plot losses\n",
    "    plt.plot(test_losses)\n",
    "    plt.show()  \n",
    "    \n",
    "    # test bleu score\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(reference, candidate):\n",
    "    \"\"\"\n",
    "    Calculate the BLEU score between a candidate and a reference.\n",
    "    \n",
    "    Args:\n",
    "        reference: a list of strings\n",
    "        candidate: a list of strings\n",
    "        \n",
    "    Returns:\n",
    "        The BLEU score\n",
    "    \"\"\"\n",
    "    bleu_scores = []\n",
    "    bleu1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "    bleu2 = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu3 = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    bleu_scores.append(bleu1)\n",
    "    bleu_scores.append(bleu2)\n",
    "    bleu_scores.append(bleu3)\n",
    "    bleu_scores.append(bleu4)\n",
    "    \n",
    "    return bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_pred_text(ret, input, label, gpt_tokenizer):\n",
    "        logits = ret.logits\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "        # add 50256 to the pred_ids first index\n",
    "        rest_of_elements = pred_ids[:, :-1]\n",
    "        last_element = pred_ids[:, -1:]\n",
    "        # 将最后一个元素拼接到剩余元素的前面\n",
    "        shifted_pred_ids = torch.cat((last_element, rest_of_elements), dim=1)\n",
    "        pred_ids = shifted_pred_ids\n",
    "\n",
    "        for input_id in input:\n",
    "                # 将Tensor转换为列表\n",
    "                input_id_list = input_id.tolist()\n",
    "                # 移除填充token ID\n",
    "                filtered_input_id_list = [tok_id for tok_id in input_id_list if tok_id != gpt_tokenizer.pad_token_id]\n",
    "                filtered_input_id_list = [tok_id for tok_id in filtered_input_id_list if tok_id != -100]\n",
    "                # 使用decode方法\n",
    "                input_text = gpt_tokenizer.decode(filtered_input_id_list, skip_special_tokens=True)\n",
    "                print(\"Input text:\", input_text)\n",
    "        # 过滤掉-100之后进行解码\n",
    "\n",
    "        filtered_pred_ids = pred_ids[label != -100]\n",
    "        print(\"filtered_pred_ids:\", filtered_pred_ids)\n",
    "        filtered_label_ids = label[label != -100]\n",
    "        print(\"filtered_label_ids:\", filtered_label_ids)\n",
    "\n",
    "\n",
    "        pred_texts = gpt_tokenizer.decode(filtered_pred_ids, skip_special_tokens=True)\n",
    "        actual_texts = gpt_tokenizer.decode(filtered_label_ids, skip_special_tokens=True)\n",
    "        \n",
    "        return actual_texts, pred_texts\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
