{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer, GPT2Config\n",
    "import copy\n",
    "\n",
    "class GPT2ModelTester():\n",
    "    def __init__(self, model, optim=torch.optim.AdamW, lr=0.001):\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.lr = lr\n",
    "        \n",
    "        \n",
    "    def _copy_model(self):\n",
    "        model_copy = GPT2Model.from_pretrained('gpt2')\n",
    "        model_copy.load_state_dict(copy.deepcopy(self.model.state_dict()))\n",
    "        \n",
    "        return model_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMLModelTester():\n",
    "    def __init__(self, model, optim=torch.optim.SGD, lr=0.01):\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.lr = lr\n",
    "\n",
    "    def _copy_model(self):\n",
    "        model_copy = nn.Sequential(OrderedDict([\n",
    "            ('l1', nn.Linear(1, 40)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('l2', nn.Linear(40, 40)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('l3', nn.Linear(40, 1))\n",
    "        ]))\n",
    "        model_copy.load_state_dict(self.model.state_dict())\n",
    "        return model_copy\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_shot_evaluation(model, model_name,  k_shot, optim,  n_samples, num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate a model using k-shot learning.\n",
    "    \n",
    "    Args:\n",
    "        model: a model that implements the k_shot_learning method\n",
    "        k_shot:  examples to use for training\n",
    "        n_samples:  examples to use for testing\n",
    "        num_episodes: the number of episodes to run\n",
    "    \"\"\"\n",
    "    if model_name == 'GPT2':\n",
    "        model = GPT2ModelTester(model, optim)\n",
    "    elif model_name == 'MAML':\n",
    "        model = MAMLModelTester(model, optim)\n",
    "        \n",
    "    model_copy = model._copy_model()\n",
    "     \n",
    "    K = len(k_shot)\n",
    "    N = len(n_samples)\n",
    "    \n",
    "    K_x = k_shot[0]\n",
    "    K_y = k_shot[1]\n",
    "    \n",
    "    N_x = n_samples[0]\n",
    "    N_y = n_samples[1]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_on_random_task(model, k_shot, K, num_steps, optim=torch.optim.SGD):\n",
    "    \"\"\"\n",
    "    trains the model on a random sine task and measures the loss curve.\n",
    "    \n",
    "    for each n in num_steps_measured, records the model function after n gradient updates.\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimiser = optim(model.parameters(), lr=0.01)\n",
    "\n",
    "    X = k_shot[0]\n",
    "    y = k_shot[1]\n",
    "    \n",
    "    losses = []\n",
    "    for step in range(1, num_steps+1):\n",
    "        loss = criterion(model(X), y) / K\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # compute grad and update inner loop weights\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_losses(initial_model, n_samples, k_shot, n_steps=10, optim=torch.optim.SGD):\n",
    "    \"\"\"\n",
    "    returns the average learning trajectory of the model trained for ``n_iterations`` over ``n_samples`` tasks\n",
    "    \"\"\"\n",
    "    K = len(k_shot)\n",
    "    x = n_samples[0]\n",
    "    avg_losses = [0] * K\n",
    "    for i in range(n_samples):\n",
    "        losses = loss_on_random_task(initial_model, K, n_steps, optim)\n",
    "        avg_losses = [l + l_new for l, l_new in zip(avg_losses, losses)]\n",
    "    avg_losses = [l / n_samples for l in avg_losses]\n",
    "    \n",
    "    return avg_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_pretrained(iterations=500):\n",
    "    \"\"\"\n",
    "    returns a model pretrained on a selection of ``iterations`` random tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set up model\n",
    "    model = nn.Sequential(OrderedDict([\n",
    "            ('l1', nn.Linear(1,40)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('l2', nn.Linear(40,40)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('l3', nn.Linear(40,1))\n",
    "        ]))\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # fit the model\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        model.zero_grad()\n",
    "        x, y = tasks.sample_task().sample_data(10)\n",
    "        loss = criterion(model(x), y)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = mixed_pretrained(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average_losses(maml.model.model, n_samples=5000, K=10), label='maml')\n",
    "plt.plot(average_losses(pretrained,       n_samples=5000, K=10), label='pretrained')\n",
    "plt.legend()\n",
    "plt.title(\"Average learning trajectory for K=10, starting from initial weights\")\n",
    "plt.xlabel(\"gradient steps taken with SGD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average_losses(maml.model.model, n_samples=5000, K=10, optim=torch.optim.Adam), label='maml')\n",
    "plt.plot(average_losses(pretrained,       n_samples=5000, K=10, optim=torch.optim.Adam), label='pretrained')\n",
    "plt.legend()\n",
    "plt.title(\"Average learning trajectory for K=10, starting from initial weights\")\n",
    "plt.xlabel(\"gradient steps taken with Adam\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_functions_at_training(initial_model, X, y, sampled_steps, x_axis, optim=torch.optim.SGD, lr=0.01):\n",
    "    \"\"\"\n",
    "    trains the model on X, y and measures the loss curve.\n",
    "    \n",
    "    for each n in sampled_steps, records model(x_axis) after n gradient updates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # copy MAML model into a new object to preserve MAML weights during training\n",
    "    model = nn.Sequential(OrderedDict([\n",
    "        ('l1', nn.Linear(1,40)),\n",
    "        ('relu1', nn.ReLU()),\n",
    "        ('l2', nn.Linear(40,40)),\n",
    "        ('relu2', nn.ReLU()),\n",
    "        ('l3', nn.Linear(40,1))\n",
    "    ]))\n",
    "    model.load_state_dict(initial_model.state_dict())\n",
    "    criterion = nn.MSELoss()\n",
    "    optimiser = optim(model.parameters(), lr)\n",
    "\n",
    "    # train model on a random task\n",
    "    num_steps = max(sampled_steps)\n",
    "    K = X.shape[0]\n",
    "    \n",
    "    losses = []\n",
    "    outputs = {}\n",
    "    for step in range(1, num_steps+1):\n",
    "        loss = criterion(model(X), y) / K\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # compute grad and update inner loop weights\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        # plot the model function\n",
    "        if step in sampled_steps:\n",
    "            outputs[step] = model(torch.tensor(x_axis, dtype=torch.float).view(-1, 1)).detach().numpy()\n",
    "            \n",
    "    outputs['initial'] = initial_model(torch.tensor(x_axis, dtype=torch.float).view(-1, 1)).detach().numpy()\n",
    "    \n",
    "    return outputs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sampled_performance(initial_model, model_name, task, X, y, optim=torch.optim.SGD, lr=0.01):\n",
    "    \n",
    "    x_axis = np.linspace(-5, 5, 1000)\n",
    "    sampled_steps=[1,10,20]\n",
    "    outputs, losses = model_functions_at_training(initial_model, \n",
    "                                                  X, y, \n",
    "                                                  sampled_steps=sampled_steps, \n",
    "                                                  x_axis=x_axis, \n",
    "                                                  optim=optim, lr=lr)\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    \n",
    "    # plot the model functions\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    plt.plot(x_axis, task.true_function(x_axis), '-', color=(0, 0, 1, 0.5), label='true function')\n",
    "    plt.scatter(X, y, label='data')\n",
    "    plt.plot(x_axis, outputs['initial'], ':', color=(0.7, 0, 0, 1), label='initial weights')\n",
    "    \n",
    "    for step in sampled_steps:\n",
    "        plt.plot(x_axis, outputs[step], \n",
    "                 '-.' if step == 1 else '-', color=(0.5, 0, 0, 1),\n",
    "                 label='model after {} steps'.format(step))\n",
    "        \n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(\"Model fit: {}\".format(model_name))\n",
    "\n",
    "    # plot losses\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Loss over time\")\n",
    "    plt.xlabel(\"gradient steps taken\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "task = tasks.sample_task()\n",
    "X, y = task.sample_data(K)\n",
    "\n",
    "plot_sampled_performance(maml.model.model, 'MAML', task, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sampled_performance(pretrained, 'pretrained at lr=0.02', task, X, y, lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "task = tasks.sample_task()\n",
    "X, y = task.sample_data(K)\n",
    "\n",
    "plot_sampled_performance(maml.model.model, 'MAML', task, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
