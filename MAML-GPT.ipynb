{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b909e49-7406-4e1b-b87e-8f7852b549db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:52.151835Z",
     "start_time": "2024-03-26T02:14:51.065430Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import transformers\n",
    "import torch\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import higher\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef63d6c-62fc-4a39-a124-b0f236297534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:53.197113Z",
     "start_time": "2024-03-26T02:14:53.185114Z"
    }
   },
   "outputs": [],
   "source": [
    "## Credits\n",
    "## This cell includes code from StylePTB (https://github.com/lvyiwei1/StylePTB/tree/master) by Yiwei Lyu et al., \n",
    "## available under the Creative Commons Attribution 4.0 International License ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)).\n",
    "\n",
    "def lowering(pairs):\n",
    "    for pair in pairs:\n",
    "        for i in range(0, 2):\n",
    "            pair[i] = pair[i].lower()\n",
    "\n",
    "def numpreprocess(pairs):\n",
    "    for pair in pairs:\n",
    "        for i in range(0, 2):\n",
    "            rep = []\n",
    "            for word in pair[i].split(' '):\n",
    "                if len(word) > 0 and word[0] in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n",
    "                    rep.append(\"NUM\")\n",
    "                else:\n",
    "                    rep.append(word)\n",
    "            pair[i] = ' '.join(rep)\n",
    "\n",
    "def padinput(inputlist, totalpad=80):\n",
    "    pads = [0] * (totalpad - len(inputlist))\n",
    "    input = inputlist + pads\n",
    "    mask = [1] * len(inputlist) + pads\n",
    "    return input, mask\n",
    "\n",
    "# create label for training\n",
    "def labels(inlen, outputlist, totalpad=80):\n",
    "    pads1 = [-100] * inlen\n",
    "    pads2 = [-100] * (totalpad - inlen - len(outputlist))\n",
    "    # print(outputlist)\n",
    "    return pads1 + outputlist + pads2\n",
    "\n",
    "def batchvalid(src, trg, batchsize):\n",
    "    validloss = 0.0\n",
    "    for i in range(0, len(src) // batchsize):\n",
    "        asrc = []\n",
    "        atrg = []\n",
    "        for pair in src[i * batchsize:(i + 1) * batchsize]:\n",
    "            asrc.append(pair)\n",
    "        for pair in trg[i * batchsize:(i + 1) * batchsize]:\n",
    "            atrg.append(pair)\n",
    "        validloss += valid(asrc, atrg)\n",
    "    return validloss / (len(src) // batchsize)\n",
    "\n",
    "def valid(src, trg):\n",
    "    padin = [padinput(l) for l in src]\n",
    "    padedin = torch.LongTensor([padin[i][0] for i in range(0, len(trg))]).to(device)\n",
    "    masks = torch.LongTensor([padin[i][1] for i in range(0, len(trg))]).to(device)\n",
    "    label = torch.LongTensor([labels(len(src[i]), trg[i]) for i in range(0, len(trg))]).to(device)\n",
    "    with torch.no_grad():\n",
    "        ret = gpt_model.forward(padedin, attention_mask=masks, labels=label)\n",
    "        loss = ret[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3caf2c59-257f-45ef-ba64-bca2d8318060",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:53.766296Z",
     "start_time": "2024-03-26T02:14:53.751233Z"
    }
   },
   "outputs": [],
   "source": [
    "class MAML_GPT():\n",
    "    def __init__(self, gpt_model, tasks, device, gpt_tokenizer=None, inner_lr=2e-4, meta_lr=2e-5, K=10, \n",
    "                 multi_batch_iter=1, inner_steps=1, early_stop=50, model_save_name=\"maml_gpt\"):\n",
    "        self.tasks = tasks\n",
    "        self.model = gpt_model\n",
    "        self.gpt_tokenizer = gpt_tokenizer\n",
    "        #self.criterion = nn.MSELoss()\n",
    "        self.meta_optimiser = optim.Adam(self.model.parameters(), meta_lr)\n",
    "        self.inner_lr = inner_lr\n",
    "        self.meta_lr = meta_lr\n",
    "        self.K = K\n",
    "        self.inner_steps = inner_steps\n",
    "        self.plot_every = 1\n",
    "        self.print_every = 1\n",
    "        self.meta_losses = []\n",
    "        self.early_stop = early_stop\n",
    "        self.meta_batch_size = len(tasks)\n",
    "        self.model_save_name = model_save_name\n",
    "        self.multi_batch_iter = multi_batch_iter\n",
    "        self.device = device\n",
    "        \n",
    "    def inner_loop(self, task):\n",
    "        \n",
    "        start_inner_time = time.time()  # 开始计时\n",
    "        if torch.cuda.is_available():\n",
    "            start_inner_mem = torch.cuda.memory_allocated()  # 获取开始时的显存使用情况\n",
    "        print(f\"Start inner loop.\")\n",
    "        print(f\"Current memory used: {(start_inner_mem) / (1024 ** 2)} MB.\")\n",
    "            \n",
    "        with higher.innerloop_ctx(self.model, self.inner_opt, copy_initial_weights=False) as (fmodel, diffopt):\n",
    "        \n",
    "            copy_time = time.time()\n",
    "            if torch.cuda.is_available():\n",
    "                copy_mem = torch.cuda.memory_allocated()  # 获取显存使用情况\n",
    "            print(f\"Copy model.\")    \n",
    "            print(f\"Time taken: {copy_time - start_inner_time} seconds; Memory used: {(copy_mem - start_inner_mem) / (1024 ** 2)} MB.\")\n",
    "        \n",
    "            random_selected_samples = random.sample(task, 2*self.K)\n",
    "            inner_train_samples = random_selected_samples[:self.K]\n",
    "            inner_test_samples = random_selected_samples[self.K:]\n",
    "            padedin, masks, labels = self.samples_to_padedin_masks_labels(inner_train_samples)\n",
    "            for step in range(self.inner_steps):\n",
    "                pred = fmodel(padedin, attention_mask=masks, labels=labels)\n",
    "                loss = pred[0]\n",
    "                diffopt.step(loss)\n",
    "            \n",
    "            padedin, masks, labels = self.samples_to_padedin_masks_labels(inner_test_samples)\n",
    "            pred = fmodel(padedin, attention_mask=masks, labels=labels)\n",
    "            loss = pred[0]\n",
    "\n",
    "            end_inner_time = time.time()\n",
    "            if torch.cuda.is_available():\n",
    "                end_inner_mem = torch.cuda.memory_allocated()  # 获取显存使用情况\n",
    "            print(f\"Finish training and testing copied model.\")    \n",
    "            print(f\"Time taken: {end_inner_time - copy_time} seconds; Memory used: {(end_inner_mem - copy_mem) / (1024 ** 2)} MB.\")\n",
    "            \n",
    "            return loss\n",
    "    \n",
    "    def main_loop(self, num_iterations):\n",
    "        min_loss = 999\n",
    "        early_stop_count = 0\n",
    "        print_loss = 0\n",
    "        for iteration in range(1, num_iterations + 1):\n",
    "\n",
    "            start_main_time = time.time()  # 开始计时\n",
    "            if torch.cuda.is_available():\n",
    "                start_main_mem = torch.cuda.memory_allocated()  # 获取开始时的显存使用情况\n",
    "            print(f\"Start main loop.\")\n",
    "            print(f\"Current memory used: {(start_main_mem) / (1024 ** 2)} MB.\")\n",
    "            \n",
    "            self.inner_opt = torch.optim.SGD(self.model.parameters(), lr=self.inner_lr)\n",
    "            meta_loss = 0\n",
    "            for _ in range(self.multi_batch_iter):\n",
    "                for task in self.tasks:\n",
    "                    meta_loss += self.inner_loop(task)\n",
    "            if meta_loss < min_loss:\n",
    "                min_loss = meta_loss\n",
    "                early_stop_count = 0\n",
    "                print(f\"New lowest loss ({meta_loss.item() / (self.meta_batch_size * self.multi_batch_iter)})found!\")\n",
    "                torch.save(self.model, f\".\\\\MAML_GPT_models\\\\{self.model_save_name}_epoch{iteration}.pt\")\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "                if early_stop_count > self.early_stop:\n",
    "                    print(f\"Early stop at epoch {iteration} because no lower loss is found in {self.early_stop} epochs\")\n",
    "                    return\n",
    "\n",
    "            finish_all_inner_time = time.time()\n",
    "            if torch.cuda.is_available():\n",
    "                finish_all_inner_mem = torch.cuda.memory_allocated()  # 获取显存使用情况\n",
    "            print(f\"Finish all inner loops in this epoch.\")    \n",
    "            print(f\"Time taken: {finish_all_inner_time - start_main_time} seconds; Memory used: {(finish_all_inner_mem - start_main_mem) / (1024 ** 2)} MB.\")\n",
    "            \n",
    "            self.meta_optimiser.zero_grad()\n",
    "            meta_loss.backward()\n",
    "            self.meta_optimiser.step()\n",
    "\n",
    "            finish_meta_update_time = time.time()\n",
    "            if torch.cuda.is_available():\n",
    "                finish_meta_update_mem = torch.cuda.memory_allocated()  # 获取显存使用情况\n",
    "            print(f\"Finish meta update in this epoch.\")    \n",
    "            print(f\"Time taken: {finish_meta_update_time - finish_all_inner_time} seconds; Memory used: {(finish_meta_update_mem - finish_all_inner_mem) / (1024 ** 2)} MB.\")\n",
    "\n",
    "            \n",
    "            print_loss += meta_loss.item() / (self.meta_batch_size * self.multi_batch_iter)\n",
    "            if iteration % self.print_every == 0:\n",
    "                print(f\"Epoch {iteration}/{num_iterations}. loss: {print_loss / self.print_every}\")\n",
    "                print_loss = 0\n",
    "            if iteration % self.plot_every == 0:\n",
    "                self.meta_losses.append(meta_loss.item() / (self.meta_batch_size * self.multi_batch_iter))\n",
    "\n",
    "    def start_train(self, epochs=10000):\n",
    "        self.epochs = epochs\n",
    "        self.main_loop(self.epochs)\n",
    "\n",
    "    def samples_to_padedin_masks_labels(self, samples):\n",
    "        src = []\n",
    "        trg = []\n",
    "        for sample in samples:\n",
    "            src.append(sample[0])\n",
    "            trg.append(sample[1])\n",
    "        padin = [self.padinput(l) for l in src]\n",
    "        padedin = torch.LongTensor([padin[i][0] for i in range(0, len(trg))]).to(self.device)\n",
    "        masks = torch.LongTensor([padin[i][1] for i in range(0, len(trg))]).to(self.device)\n",
    "        labels = torch.LongTensor([self.create_labels(len(src[i]), trg[i]) for i in range(0, len(trg))]).to(self.device)\n",
    "        return padedin, masks, labels\n",
    "\n",
    "    def create_labels(self, inlen, outputlist, totalpad=80):\n",
    "        pads1 = [-100] * inlen\n",
    "        pads2 = [-100] * (totalpad - inlen - len(outputlist))\n",
    "        return pads1 + outputlist + pads2\n",
    "    \n",
    "    def padinput(self, inputlist, totalpad=80):\n",
    "        pads = [0] * (totalpad - len(inputlist))\n",
    "        input = inputlist + pads\n",
    "        mask = [1] * len(inputlist) + pads\n",
    "        return input, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e9f4364-1c2c-44cf-9515-10c5a39252f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:57.207614Z",
     "start_time": "2024-03-26T02:14:53.939349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "# Pre-process the data\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Device: {device}\")\n",
    "gpt_tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "meta_train_task_name_list = [\"ARR\", \"TFU\", \"ATP\", \"PPR\", \"TPA\"]\n",
    "meta_test_task_name_list = [\"PTA\", \"SBR\", \"TPR\"]\n",
    "\n",
    "train_task_pair_list = [] # Nested list with pairs from different tasks\n",
    "for meta_train_task_name in meta_train_task_name_list:\n",
    "    f = open(f'.\\\\Data\\\\Meta_training\\\\{meta_train_task_name}\\\\train.tsv', 'r')\n",
    "    ff = csv.reader(f, delimiter='\\t')\n",
    "    pairs = []\n",
    "    for row in ff:\n",
    "        pairs.append(row)\n",
    "    lowering(pairs)\n",
    "    numpreprocess(pairs)\n",
    "    pairsEncode = []\n",
    "    for i in pairs:\n",
    "        pairsEncode.append((gpt_tokenizer.encode(i[0] + \" <|endoftext|>\"), gpt_tokenizer.encode(i[1] + \" <|endoftext|>\")))\n",
    "    train_task_pair_list.append(pairsEncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d6ae7e-b65e-4ba6-b546-4447a3bdac2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:57.238728Z",
     "start_time": "2024-03-26T02:14:57.220617Z"
    }
   },
   "outputs": [],
   "source": [
    "maml_gpt = MAML_GPT(gpt_model=gpt_model, tasks=train_task_pair_list, device = device, gpt_tokenizer=gpt_tokenizer, \n",
    "                    multi_batch_iter=1, early_stop=50, model_save_name=\"maml_gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d543cd5-c31d-4dd3-a94b-863ba853e8f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:30:28.857492Z",
     "start_time": "2024-03-26T02:30:01.736221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lowest loss found!\n",
      "New lowest loss found!\n",
      "New lowest loss found!\n",
      "New lowest loss found!\n",
      "New lowest loss found!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File .\\MAML_GPT_models\\maml_gpt_epoch5.pt cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmaml_gpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_train\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [3], line 71\u001b[0m, in \u001b[0;36mMAML_GPT.start_train\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m epochs\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [3], line 52\u001b[0m, in \u001b[0;36mMAML_GPT.main_loop\u001b[1;34m(self, num_iterations)\u001b[0m\n\u001b[0;32m     50\u001b[0m     early_stop_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew lowest loss found!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mMAML_GPT_models\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_save_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_epoch\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43miteration\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     early_stop_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\mlp\\lib\\site-packages\\torch\\serialization.py:422\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    419\u001b[0m _check_dill_version(pickle_module)\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    423\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\mlp\\lib\\site-packages\\torch\\serialization.py:309\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Anaconda\\envs\\mlp\\lib\\site-packages\\torch\\serialization.py:287\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_zipfile_writer_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File .\\MAML_GPT_models\\maml_gpt_epoch5.pt cannot be opened."
     ]
    }
   ],
   "source": [
    "maml_gpt.start_train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431024c-804e-4902-83dd-079967e05375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
