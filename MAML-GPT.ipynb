{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b909e49-7406-4e1b-b87e-8f7852b549db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:52.151835Z",
     "start_time": "2024-03-26T02:14:51.065430Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\Grid\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import transformers\n",
    "import torch\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import higher\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef63d6c-62fc-4a39-a124-b0f236297534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:53.197113Z",
     "start_time": "2024-03-26T02:14:53.185114Z"
    }
   },
   "outputs": [],
   "source": [
    "## Credits\n",
    "## This cell includes code from StylePTB (https://github.com/lvyiwei1/StylePTB/tree/master) by Yiwei Lyu et al., \n",
    "## available under the Creative Commons Attribution 4.0 International License ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)).\n",
    "\n",
    "def lowering(pairs):\n",
    "    for pair in pairs:\n",
    "        for i in range(0, 2):\n",
    "            pair[i] = pair[i].lower()\n",
    "\n",
    "def numpreprocess(pairs):\n",
    "    for pair in pairs:\n",
    "        for i in range(0, 2):\n",
    "            rep = []\n",
    "            for word in pair[i].split(' '):\n",
    "                if len(word) > 0 and word[0] in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n",
    "                    rep.append(\"NUM\")\n",
    "                else:\n",
    "                    rep.append(word)\n",
    "            pair[i] = ' '.join(rep)\n",
    "\n",
    "def padinput(inputlist, totalpad=80):\n",
    "    pads = [0] * (totalpad - len(inputlist))\n",
    "    input = inputlist + pads\n",
    "    mask = [1] * len(inputlist) + pads\n",
    "    return input, mask\n",
    "\n",
    "# create label for training\n",
    "def labels(inlen, outputlist, totalpad=80):\n",
    "    pads1 = [-100] * inlen\n",
    "    pads2 = [-100] * (totalpad - inlen - len(outputlist))\n",
    "    # print(outputlist)\n",
    "    return pads1 + outputlist + pads2\n",
    "\n",
    "def batchvalid(src, trg, batchsize):\n",
    "    validloss = 0.0\n",
    "    for i in range(0, len(src) // batchsize):\n",
    "        asrc = []\n",
    "        atrg = []\n",
    "        for pair in src[i * batchsize:(i + 1) * batchsize]:\n",
    "            asrc.append(pair)\n",
    "        for pair in trg[i * batchsize:(i + 1) * batchsize]:\n",
    "            atrg.append(pair)\n",
    "        validloss += valid(asrc, atrg)\n",
    "    return validloss / (len(src) // batchsize)\n",
    "\n",
    "def valid(src, trg):\n",
    "    padin = [padinput(l) for l in src]\n",
    "    padedin = torch.LongTensor([padin[i][0] for i in range(0, len(trg))]).to(device)\n",
    "    masks = torch.LongTensor([padin[i][1] for i in range(0, len(trg))]).to(device)\n",
    "    label = torch.LongTensor([labels(len(src[i]), trg[i]) for i in range(0, len(trg))]).to(device)\n",
    "    with torch.no_grad():\n",
    "        ret = gpt_model.forward(padedin, attention_mask=masks, labels=label)\n",
    "        loss = ret[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3caf2c59-257f-45ef-ba64-bca2d8318060",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:53.766296Z",
     "start_time": "2024-03-26T02:14:53.751233Z"
    }
   },
   "outputs": [],
   "source": [
    "class MAML_GPT():\n",
    "    def __init__(self, gpt_model, tasks, device, gpt_tokenizer=None, inner_lr=2e-4, meta_lr=2e-5, K=10, \n",
    "                 multi_batch_iter=1, inner_steps=1, early_stop=50, model_save_name=\"maml_gpt\"):\n",
    "        self.tasks = tasks\n",
    "        self.model = gpt_model\n",
    "        self.gpt_tokenizer = gpt_tokenizer\n",
    "        #self.criterion = nn.MSELoss()\n",
    "        self.meta_optimiser = optim.Adam(self.model.parameters(), meta_lr)\n",
    "        self.inner_lr = inner_lr\n",
    "        self.meta_lr = meta_lr\n",
    "        self.K = K\n",
    "        self.inner_steps = inner_steps\n",
    "        self.plot_every = 1\n",
    "        self.print_every = 1\n",
    "        self.meta_losses = []\n",
    "        self.early_stop = early_stop\n",
    "        self.meta_batch_size = len(tasks)\n",
    "        self.model_save_name = model_save_name\n",
    "        self.multi_batch_iter = multi_batch_iter\n",
    "        self.device = device\n",
    "        \n",
    "    def inner_loop(self, task):\n",
    "        \n",
    "        start_inner_time = time.time()  # 开始计时\n",
    "        if torch.cuda.is_available():\n",
    "            start_inner_mem = torch.cuda.memory_allocated()  # 获取开始时的显存使用情况\n",
    "        print(f\"Start inner loop.\")\n",
    "        print(f\"Current memory used: {(start_inner_mem) / (1024 ** 2)} MB.\")\n",
    "            \n",
    "        with higher.innerloop_ctx(self.model, self.inner_opt, copy_initial_weights=False) as (fmodel, diffopt):\n",
    "        \n",
    "            copy_time = time.time()\n",
    "            if torch.cuda.is_available():\n",
    "                copy_mem = torch.cuda.memory_allocated()  # 获取显存使用情况\n",
    "            print(f\"Copy model.\")    \n",
    "            print(f\"Time taken: {copy_time - start_inner_time} seconds; Memory used: {(copy_mem - start_inner_mem) / (1024 ** 2)} MB.\")\n",
    "        \n",
    "            random_selected_samples = random.sample(task, 2*self.K)\n",
    "            inner_train_samples = random_selected_samples[:self.K]\n",
    "            inner_test_samples = random_selected_samples[self.K:]\n",
    "            padedin, masks, labels = self.samples_to_padedin_masks_labels(inner_train_samples)\n",
    "            for step in range(self.inner_steps):\n",
    "                pred = fmodel(padedin, attention_mask=masks, labels=labels)\n",
    "                loss = pred[0]\n",
    "                diffopt.step(loss)\n",
    "            \n",
    "            padedin, masks, labels = self.samples_to_padedin_masks_labels(inner_test_samples)\n",
    "            pred = fmodel(padedin, attention_mask=masks, labels=labels)\n",
    "            loss = pred[0]\n",
    "\n",
    "            end_inner_time = time.time()\n",
    "            if torch.cuda.is_available():\n",
    "                end_inner_mem = torch.cuda.memory_allocated()  # 获取显存使用情况\n",
    "            print(f\"Finish training and testing copied model.\")    \n",
    "            print(f\"Time taken: {end_inner_time - copy_time} seconds; Memory used: {(end_inner_mem - copy_mem) / (1024 ** 2)} MB.\")\n",
    "            \n",
    "            return loss\n",
    "    \n",
    "    def main_loop(self, num_iterations):\n",
    "        min_loss = 999\n",
    "        early_stop_count = 0\n",
    "        print_loss = 0\n",
    "        for iteration in range(1, num_iterations + 1):\n",
    "\n",
    "            start_main_time = time.time()  # 开始计时\n",
    "            if torch.cuda.is_available():\n",
    "                start_main_mem = torch.cuda.memory_allocated()  # 获取开始时的显存使用情况\n",
    "            print()\n",
    "            print(f\"Start main loop epoch {iteration}.\")\n",
    "            print(f\"Current memory used: {(start_main_mem) / (1024 ** 2)} MB.\")\n",
    "            \n",
    "            self.inner_opt = torch.optim.SGD(self.model.parameters(), lr=self.inner_lr)\n",
    "            meta_loss = 0\n",
    "            for _ in range(self.multi_batch_iter):\n",
    "                for task in self.tasks:\n",
    "                    meta_loss += self.inner_loop(task)\n",
    "            if meta_loss < min_loss:\n",
    "                min_loss = meta_loss\n",
    "                early_stop_count = 0\n",
    "                print(f\"New lowest loss ({meta_loss.item() / (self.meta_batch_size * self.multi_batch_iter)})found!\")\n",
    "                torch.save(self.model, f\".\\\\MAML_GPT_models\\\\{self.model_save_name}_epoch{iteration}.pt\")\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "                if early_stop_count > self.early_stop:\n",
    "                    print(f\"Early stop at epoch {iteration} because no lower loss is found in {self.early_stop} epochs\")\n",
    "                    return\n",
    "\n",
    "            finish_all_inner_time = time.time()\n",
    "            if torch.cuda.is_available():\n",
    "                finish_all_inner_mem = torch.cuda.memory_allocated()  # 获取显存使用情况\n",
    "            print(f\"Finish all inner loops in this epoch.\")    \n",
    "            print(f\"Time taken: {finish_all_inner_time - start_main_time} seconds; Memory used: {(finish_all_inner_mem - start_main_mem) / (1024 ** 2)} MB.\")\n",
    "            \n",
    "            self.meta_optimiser.zero_grad()\n",
    "            meta_loss.backward()\n",
    "            self.meta_optimiser.step()\n",
    "\n",
    "            finish_meta_update_time = time.time()\n",
    "            if torch.cuda.is_available():\n",
    "                finish_meta_update_mem = torch.cuda.memory_allocated()  # 获取显存使用情况\n",
    "            print(f\"Finish meta update in this epoch.\")    \n",
    "            print(f\"Time taken: {finish_meta_update_time - finish_all_inner_time} seconds; Memory used: {(finish_meta_update_mem - finish_all_inner_mem) / (1024 ** 2)} MB.\")\n",
    "            \n",
    "            print_loss += meta_loss.item() / (self.meta_batch_size * self.multi_batch_iter)\n",
    "            if iteration % self.print_every == 0:\n",
    "                print(f\"Epoch {iteration}/{num_iterations}. loss: {print_loss / self.print_every}\")\n",
    "                print_loss = 0\n",
    "            if iteration % self.plot_every == 0:\n",
    "                self.meta_losses.append(meta_loss.item() / (self.meta_batch_size * self.multi_batch_iter))\n",
    "\n",
    "    def start_train(self, epochs=10000):\n",
    "        self.epochs = epochs\n",
    "        self.main_loop(self.epochs)\n",
    "\n",
    "    def samples_to_padedin_masks_labels(self, samples):\n",
    "        src = []\n",
    "        trg = []\n",
    "        for sample in samples:\n",
    "            src.append(sample[0])\n",
    "            trg.append(sample[1])\n",
    "        padin = [self.padinput(l) for l in src]\n",
    "        padedin = torch.LongTensor([padin[i][0] for i in range(0, len(trg))]).to(self.device)\n",
    "        masks = torch.LongTensor([padin[i][1] for i in range(0, len(trg))]).to(self.device)\n",
    "        labels = torch.LongTensor([self.create_labels(len(src[i]), trg[i]) for i in range(0, len(trg))]).to(self.device)\n",
    "        return padedin, masks, labels\n",
    "\n",
    "    def create_labels(self, inlen, outputlist, totalpad=80):\n",
    "        pads1 = [-100] * inlen\n",
    "        pads2 = [-100] * (totalpad - inlen - len(outputlist))\n",
    "        return pads1 + outputlist + pads2\n",
    "    \n",
    "    def padinput(self, inputlist, totalpad=80):\n",
    "        pads = [0] * (totalpad - len(inputlist))\n",
    "        input = inputlist + pads\n",
    "        mask = [1] * len(inputlist) + pads\n",
    "        return input, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e9f4364-1c2c-44cf-9515-10c5a39252f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:57.207614Z",
     "start_time": "2024-03-26T02:14:53.939349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "# Pre-process the data\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Device: {device}\")\n",
    "gpt_tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "meta_train_task_name_list = [\"ARR\", \"TFU\", \"ATP\", \"PPR\", \"TPA\"]\n",
    "meta_test_task_name_list = [\"PTA\", \"SBR\", \"TPR\"]\n",
    "\n",
    "train_task_pair_list = [] # Nested list with pairs from different tasks\n",
    "for meta_train_task_name in meta_train_task_name_list:\n",
    "    f = open(f'.\\\\Data\\\\Meta_training\\\\{meta_train_task_name}\\\\train.tsv', 'r')\n",
    "    ff = csv.reader(f, delimiter='\\t')\n",
    "    pairs = []\n",
    "    for row in ff:\n",
    "        pairs.append(row)\n",
    "    lowering(pairs)\n",
    "    numpreprocess(pairs)\n",
    "    pairsEncode = []\n",
    "    for i in pairs:\n",
    "        pairsEncode.append((gpt_tokenizer.encode(i[0] + \" <|endoftext|>\"), gpt_tokenizer.encode(i[1] + \" <|endoftext|>\")))\n",
    "    train_task_pair_list.append(pairsEncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d6ae7e-b65e-4ba6-b546-4447a3bdac2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:57.238728Z",
     "start_time": "2024-03-26T02:14:57.220617Z"
    }
   },
   "outputs": [],
   "source": [
    "maml_gpt = MAML_GPT(gpt_model=gpt_model, tasks=train_task_pair_list, device = device, gpt_tokenizer=gpt_tokenizer, \n",
    "                    multi_batch_iter=1, early_stop=50, model_save_name=\"maml_gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d543cd5-c31d-4dd3-a94b-863ba853e8f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:30:28.857492Z",
     "start_time": "2024-03-26T02:30:01.736221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start main loop.\n",
      "Current memory used: 487.46875 MB.\n",
      "Start inner loop.\n",
      "Current memory used: 487.46875 MB.\n",
      "Copy model.\n",
      "Time taken: 0.04510021209716797 seconds; Memory used: 487.46875 MB.\n",
      "Finish training and testing copied model.\n",
      "Time taken: 3.8702666759490967 seconds; Memory used: 3872.755859375 MB.\n",
      "Start inner loop.\n",
      "Current memory used: 4602.4951171875 MB.\n",
      "Copy model.\n",
      "Time taken: 0.24521231651306152 seconds; Memory used: 491.4375 MB.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 7.01 GiB is allocated by PyTorch, and 296.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmaml_gpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_train\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 113\u001b[0m, in \u001b[0;36mMAML_GPT.start_train\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m):\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m epochs\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 75\u001b[0m, in \u001b[0;36mMAML_GPT.main_loop\u001b[1;34m(self, num_iterations)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_batch_iter):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks:\n\u001b[1;32m---> 75\u001b[0m         meta_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta_loss \u001b[38;5;241m<\u001b[39m min_loss:\n\u001b[0;32m     77\u001b[0m     min_loss \u001b[38;5;241m=\u001b[39m meta_loss\n",
      "Cell \u001b[1;32mIn[3], line 45\u001b[0m, in \u001b[0;36mMAML_GPT.inner_loop\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m     43\u001b[0m     pred \u001b[38;5;241m=\u001b[39m fmodel(padedin, attention_mask\u001b[38;5;241m=\u001b[39mmasks, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m     44\u001b[0m     loss \u001b[38;5;241m=\u001b[39m pred[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 45\u001b[0m     \u001b[43mdiffopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m padedin, masks, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples_to_padedin_masks_labels(inner_test_samples)\n\u001b[0;32m     48\u001b[0m pred \u001b[38;5;241m=\u001b[39m fmodel(padedin, attention_mask\u001b[38;5;241m=\u001b[39mmasks, labels\u001b[38;5;241m=\u001b[39mlabels)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\Grid\\Lib\\site-packages\\higher\\optim.py:229\u001b[0m, in \u001b[0;36mDifferentiableOptimizer.step\u001b[1;34m(self, loss, params, override, grad_callback, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# This allows us to gracefully deal with cases where params are frozen.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m grad_targets \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    225\u001b[0m     p \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;28;01melse\u001b[39;00m _torch\u001b[38;5;241m.\u001b[39mtensor([], requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params\n\u001b[0;32m    227\u001b[0m ]\n\u001b[1;32m--> 229\u001b[0m all_grads \u001b[38;5;241m=\u001b[39m \u001b[43m_torch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_track_higher_grads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# boo\u001b[39;49;00m\n\u001b[0;32m    234\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     all_grads \u001b[38;5;241m=\u001b[39m grad_callback(all_grads)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\Grid\\Lib\\site-packages\\torch\\autograd\\__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    408\u001b[0m         grad_outputs_\n\u001b[0;32m    409\u001b[0m     )\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 411\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    422\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    424\u001b[0m     ):\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 7.01 GiB is allocated by PyTorch, and 296.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "maml_gpt.start_train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431024c-804e-4902-83dd-079967e05375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
