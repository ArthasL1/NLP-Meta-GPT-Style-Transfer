{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b909e49-7406-4e1b-b87e-8f7852b549db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:52.151835Z",
     "start_time": "2024-03-26T02:14:51.065430Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\Grid\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import transformers\n",
    "import torch\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import higher\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef63d6c-62fc-4a39-a124-b0f236297534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:53.197113Z",
     "start_time": "2024-03-26T02:14:53.185114Z"
    }
   },
   "outputs": [],
   "source": [
    "## Credits\n",
    "## This cell includes code from StylePTB (https://github.com/lvyiwei1/StylePTB/tree/master) by Yiwei Lyu et al., \n",
    "## available under the Creative Commons Attribution 4.0 International License ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)).\n",
    "\n",
    "def lowering(pairs):\n",
    "    for pair in pairs:\n",
    "        for i in range(0, 2):\n",
    "            pair[i] = pair[i].lower()\n",
    "\n",
    "def numpreprocess(pairs):\n",
    "    for pair in pairs:\n",
    "        for i in range(0, 2):\n",
    "            rep = []\n",
    "            for word in pair[i].split(' '):\n",
    "                if len(word) > 0 and word[0] in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n",
    "                    rep.append(\"NUM\")\n",
    "                else:\n",
    "                    rep.append(word)\n",
    "            pair[i] = ' '.join(rep)\n",
    "\n",
    "def padinput(inputlist, totalpad=80):\n",
    "    pads = [0] * (totalpad - len(inputlist))\n",
    "    input = inputlist + pads\n",
    "    mask = [1] * len(inputlist) + pads\n",
    "    return input, mask\n",
    "\n",
    "# create label for training\n",
    "def labels(inlen, outputlist, totalpad=80):\n",
    "    pads1 = [-100] * inlen\n",
    "    pads2 = [-100] * (totalpad - inlen - len(outputlist))\n",
    "    # print(outputlist)\n",
    "    return pads1 + outputlist + pads2\n",
    "\n",
    "def batchvalid(src, trg, batchsize):\n",
    "    validloss = 0.0\n",
    "    for i in range(0, len(src) // batchsize):\n",
    "        asrc = []\n",
    "        atrg = []\n",
    "        for pair in src[i * batchsize:(i + 1) * batchsize]:\n",
    "            asrc.append(pair)\n",
    "        for pair in trg[i * batchsize:(i + 1) * batchsize]:\n",
    "            atrg.append(pair)\n",
    "        validloss += valid(asrc, atrg)\n",
    "    return validloss / (len(src) // batchsize)\n",
    "\n",
    "def valid(src, trg):\n",
    "    padin = [padinput(l) for l in src]\n",
    "    padedin = torch.LongTensor([padin[i][0] for i in range(0, len(trg))]).to(device)\n",
    "    masks = torch.LongTensor([padin[i][1] for i in range(0, len(trg))]).to(device)\n",
    "    label = torch.LongTensor([labels(len(src[i]), trg[i]) for i in range(0, len(trg))]).to(device)\n",
    "    with torch.no_grad():\n",
    "        ret = gpt_model.forward(padedin, attention_mask=masks, labels=label)\n",
    "        loss = ret[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3caf2c59-257f-45ef-ba64-bca2d8318060",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:53.766296Z",
     "start_time": "2024-03-26T02:14:53.751233Z"
    }
   },
   "outputs": [],
   "source": [
    "class MAML_GPT():\n",
    "    def __init__(self, gpt_model, tasks, device, gpt_tokenizer=None, inner_lr=2e-4, meta_lr=2e-5, K=10, query_size=10, \n",
    "                 multi_batch_iter=1, inner_steps=1, early_stop=300, model_save_name=\"maml_gpt\", print_time_mem=False):\n",
    "        self.tasks = tasks\n",
    "        self.model = gpt_model\n",
    "        self.gpt_tokenizer = gpt_tokenizer\n",
    "        #self.criterion = nn.MSELoss()\n",
    "        self.meta_optimiser = optim.Adam(self.model.parameters(), meta_lr)\n",
    "        self.inner_lr = inner_lr\n",
    "        self.meta_lr = meta_lr\n",
    "        self.K = K\n",
    "        self.inner_steps = inner_steps\n",
    "        self.plot_every = 10\n",
    "        self.print_every = 10\n",
    "        self.meta_losses = []\n",
    "        self.early_stop = early_stop\n",
    "        self.meta_batch_size = len(tasks)\n",
    "        self.model_save_name = model_save_name\n",
    "        self.multi_batch_iter = multi_batch_iter\n",
    "        self.device = device\n",
    "        self.query_size = query_size\n",
    "        self.print_time_mem = print_time_mem\n",
    "        \n",
    "    def inner_loop(self, task):\n",
    "        self.inner_loop_counter += 1\n",
    "\n",
    "        if self.print_time_mem:\n",
    "            start_inner_time = time.time()  \n",
    "            if torch.cuda.is_available():\n",
    "                start_inner_mem = torch.cuda.memory_allocated()\n",
    "            print(f\"Start inner loop {self.inner_loop_counter}.\")\n",
    "            print(f\"Current memory used: {(start_inner_mem) / (1024 ** 2)} MB.\")\n",
    "    \n",
    "        with higher.innerloop_ctx(self.model, self.inner_opt, copy_initial_weights=False) as (fmodel, diffopt):\n",
    "\n",
    "            if self.print_time_mem:\n",
    "                copy_time = time.time()\n",
    "                if torch.cuda.is_available():\n",
    "                    copy_mem = torch.cuda.memory_allocated() \n",
    "                print(f\"Copy model.\")    \n",
    "                print(f\"Time taken: {copy_time - start_inner_time} seconds; Memory used: {(copy_mem - start_inner_mem) / (1024 ** 2)} MB.\")\n",
    "        \n",
    "            random_selected_samples = random.sample(task, self.K + self.query_size)\n",
    "            inner_train_samples = random_selected_samples[:self.K]\n",
    "            inner_test_samples = random_selected_samples[self.K:]\n",
    "            padedin, masks, labels = self.samples_to_padedin_masks_labels(inner_train_samples)\n",
    "            for step in range(self.inner_steps):\n",
    "                pred = fmodel(padedin, attention_mask=masks, labels=labels)\n",
    "                loss_inner_step = pred[0]\n",
    "                diffopt.step(loss_inner_step)\n",
    "                del pred\n",
    "\n",
    "            if self.print_time_mem:\n",
    "                end_inner_train_time = time.time()\n",
    "                if torch.cuda.is_available():\n",
    "                    end_inner_train_mem = torch.cuda.memory_allocated() \n",
    "                print(f\"Finish training copied model.\")    \n",
    "                print(f\"Time taken: {end_inner_train_time - copy_time} seconds; Memory used: {(end_inner_train_mem - copy_mem) / (1024 ** 2)} MB.\")\n",
    "        \n",
    "            padedin, masks, labels = self.samples_to_padedin_masks_labels(inner_test_samples)\n",
    "            pred = fmodel(padedin, attention_mask=masks, labels=labels)\n",
    "            loss = pred[0]\n",
    "            loss.backward()\n",
    "            loss_item = loss.detach().item()\n",
    "\n",
    "            if self.print_time_mem:\n",
    "                end_inner_test_time = time.time()\n",
    "                if torch.cuda.is_available():\n",
    "                    end_inner_test_mem = torch.cuda.memory_allocated()\n",
    "                print(f\"Finish testing copied model.\")    \n",
    "                print(f\"Time taken: {end_inner_test_time - end_inner_train_time} seconds; Memory used: {(end_inner_test_mem - end_inner_train_mem) / (1024 ** 2)} MB.\")\n",
    "\n",
    "            del pred\n",
    "            del fmodel\n",
    "            del diffopt\n",
    "            \n",
    "            if self.print_time_mem:\n",
    "                end_inner_del_time = time.time()\n",
    "                if torch.cuda.is_available():\n",
    "                    end_inner_del_mem = torch.cuda.memory_allocated() \n",
    "                print(f\"Release Memory.\")    \n",
    "                print(f\"Time taken: {end_inner_del_time - end_inner_test_time} seconds; Memory used: {(end_inner_del_mem - end_inner_test_mem) / (1024 ** 2)} MB.\")\n",
    "\n",
    "            return loss_item\n",
    "    \n",
    "    def main_loop(self, num_iterations):\n",
    "        min_loss = 999\n",
    "        min_epoch = -1\n",
    "        early_stop_count = 0\n",
    "        print_loss = 0\n",
    "        plot_loss = 0\n",
    "        saved_models = []\n",
    "        folder_path = f\".\\\\MAML_GPT_models\\\\{model_save_name}\"\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        for iteration in range(1, num_iterations + 1):\n",
    "            self.main_loop_counter = iteration\n",
    "\n",
    "            if self.print_time_mem:\n",
    "                start_main_time = time.time()\n",
    "                if torch.cuda.is_available():\n",
    "                    start_main_mem = torch.cuda.memory_allocated() \n",
    "                print()\n",
    "                print(f\"Start main loop epoch {iteration}.\")\n",
    "                print(f\"Current memory used: {(start_main_mem) / (1024 ** 2)} MB.\")\n",
    "\n",
    "            self.meta_optimiser.zero_grad()\n",
    "            self.inner_opt = torch.optim.SGD(self.model.parameters(), lr=self.inner_lr)\n",
    "            meta_loss = 0\n",
    "            self.inner_loop_counter = 0\n",
    "            for _ in range(self.multi_batch_iter):\n",
    "                for task in self.tasks:\n",
    "                    meta_loss += self.inner_loop(task)\n",
    "            meta_loss = meta_loss / (self.meta_batch_size * self.multi_batch_iter)\n",
    "            if meta_loss < min_loss:\n",
    "                min_loss = meta_loss\n",
    "                min_epoch = iteration\n",
    "                early_stop_count = 0\n",
    "                #print(f\"New lowest loss ({meta_loss}) found at Epoch {iteration}!\")\n",
    "                save_path = f\"{folder_path}\\\\bestMetaLoss_epoch{iteration}.pt\"\n",
    "                torch.save(self.model, save_path)\n",
    "                saved_models.append(save_path)\n",
    "                if len(saved_models) > 5:\n",
    "                    oldest_model = saved_models.pop(0) \n",
    "                    os.remove(oldest_model)\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "                if early_stop_count > self.early_stop:\n",
    "                    print(f\"Early stop at epoch {iteration} because no lower loss is found in {self.early_stop} epochs\")\n",
    "                    print(f\"The lowest loss: {min_loss}\")\n",
    "                    print(f\"The lowest loss found at Epoch {min_epoch}\")\n",
    "                    return\n",
    "\n",
    "            if self.print_time_mem:\n",
    "                finish_all_inner_time = time.time()\n",
    "                if torch.cuda.is_available():\n",
    "                    finish_all_inner_mem = torch.cuda.memory_allocated()\n",
    "                print(f\"Finish all inner loops in this epoch.\")    \n",
    "                print(f\"Time taken: {finish_all_inner_time - start_main_time} seconds; Memory used: {(finish_all_inner_mem - start_main_mem) / (1024 ** 2)} MB.\")\n",
    "            \n",
    "            #self.meta_optimiser.zero_grad()\n",
    "            #meta_loss.backward()\n",
    "            self.meta_optimiser.step()\n",
    "\n",
    "            if self.print_time_mem:\n",
    "                finish_meta_update_time = time.time()\n",
    "                if torch.cuda.is_available():\n",
    "                    finish_meta_update_mem = torch.cuda.memory_allocated() \n",
    "                print(f\"Finish meta update in this epoch.\")    \n",
    "                print(f\"Time taken: {finish_meta_update_time - finish_all_inner_time} seconds; Memory used: {(finish_meta_update_mem - finish_all_inner_mem) / (1024 ** 2)} MB.\")\n",
    "            \n",
    "            print_loss += meta_loss \n",
    "            plot_loss += meta_loss \n",
    "            if iteration % self.print_every == 0:\n",
    "                print(f\"Epoch {iteration}/{num_iterations}. loss: {print_loss / self.print_every}\")\n",
    "                print_loss = 0\n",
    "            if iteration % self.plot_every == 0:\n",
    "                self.meta_losses.append(plot_loss / self.plot_every)\n",
    "                plot_loss = 0\n",
    "        print(f\"The lowest loss: {min_loss}\")\n",
    "        print(f\"The lowest loss is found at Epoch {min_epoch}\")\n",
    "\n",
    "    def start_train(self, epochs=10000):\n",
    "        self.epochs = epochs\n",
    "        self.main_loop(self.epochs)\n",
    "\n",
    "    def samples_to_padedin_masks_labels(self, samples):\n",
    "        src = []\n",
    "        trg = []\n",
    "        for sample in samples:\n",
    "            src.append(sample[0])\n",
    "            trg.append(sample[1])\n",
    "        padin = [self.padinput(l) for l in src]\n",
    "        padedin = torch.LongTensor([padin[i][0] for i in range(0, len(trg))]).to(self.device)\n",
    "        masks = torch.LongTensor([padin[i][1] for i in range(0, len(trg))]).to(self.device)\n",
    "        labels = torch.LongTensor([self.create_labels(len(src[i]), trg[i]) for i in range(0, len(trg))]).to(self.device)\n",
    "        return padedin, masks, labels\n",
    "\n",
    "    def create_labels(self, inlen, outputlist, totalpad=80):\n",
    "        pads1 = [-100] * inlen\n",
    "        pads2 = [-100] * (totalpad - inlen - len(outputlist))\n",
    "        return pads1 + outputlist + pads2\n",
    "    \n",
    "    def padinput(self, inputlist, totalpad=80):\n",
    "        pads = [0] * (totalpad - len(inputlist))\n",
    "        input = inputlist + pads\n",
    "        mask = [1] * len(inputlist) + pads\n",
    "        return input, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e9f4364-1c2c-44cf-9515-10c5a39252f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:57.207614Z",
     "start_time": "2024-03-26T02:14:53.939349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\Data\\\\Meta_training\\\\ARR\\\\train.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m train_task_pair_list \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# Nested list with pairs from different tasks\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m meta_train_task_name \u001b[38;5;129;01min\u001b[39;00m meta_train_task_name_list:\n\u001b[1;32m---> 16\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mData\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mMeta_training\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmeta_train_task_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mtrain.tsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     ff \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(f, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m     pairs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\Grid\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.\\\\Data\\\\Meta_training\\\\ARR\\\\train.tsv'"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "# Pre-process the data\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Device: {device}\")\n",
    "gpt_tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "meta_train_task_name_list = [\"ARR\", \"TFU\", \"ATP\", \"PPR\", \"TPA\"]\n",
    "meta_test_task_name_list = [\"PTA\", \"SBR\", \"TPR\"]\n",
    "\n",
    "train_task_pair_list = [] # Nested list with pairs from different tasks\n",
    "for meta_train_task_name in meta_train_task_name_list:\n",
    "    f = open(f'.\\\\Dataset_1\\\\Meta_training\\\\{meta_train_task_name}\\\\train.tsv', 'r')\n",
    "    ff = csv.reader(f, delimiter='\\t')\n",
    "    pairs = []\n",
    "    for row in ff:\n",
    "        pairs.append(row)\n",
    "    lowering(pairs)\n",
    "    numpreprocess(pairs)\n",
    "    pairsEncode = []\n",
    "    for i in pairs:\n",
    "        pairsEncode.append((gpt_tokenizer.encode(i[0] + \" <|endoftext|>\"), gpt_tokenizer.encode(i[1] + \" <|endoftext|>\")))\n",
    "    train_task_pair_list.append(pairsEncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d6ae7e-b65e-4ba6-b546-4447a3bdac2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:57.238728Z",
     "start_time": "2024-03-26T02:14:57.220617Z"
    }
   },
   "outputs": [],
   "source": [
    "maml_gpt = MAML_GPT(gpt_model=gpt_model, tasks=train_task_pair_list, device = device, gpt_tokenizer=gpt_tokenizer, \n",
    "                    multi_batch_iter=1, query_size=10, model_save_name=\"maml_gpt\", print_time_mem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d543cd5-c31d-4dd3-a94b-863ba853e8f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T02:30:28.857492Z",
     "start_time": "2024-03-26T02:30:01.736221Z"
    }
   },
   "outputs": [],
   "source": [
    "maml_gpt.start_train(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd90ae-e0cd-4964-b388-eedae900a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(maml_gpt.meta_losses) + 1), maml_gpt.meta_losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title('MAML-GPT Learning Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118c87b8-a8fd-46b8-ac5c-2126673475d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb45cf9-d362-4173-b0a9-5c5fba3f2a10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
